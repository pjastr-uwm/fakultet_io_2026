{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pjastr-uwm/fakultet_io_2026/blob/main/lab01/tokenizacja_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3LumHQcLFdY"
      },
      "source": [
        "# Tokenizacja tekstu za pomoca pakietu NLTK\n",
        "\n",
        "## Przewodnik praktyczny\n",
        "\n",
        "**Wymagania wstepne:** podstawowa znajomosc Pythona (zmienne, listy, petle)\n",
        "\n",
        "---\n",
        "\n",
        "### Czym jest ten notatnik?\n",
        "\n",
        "Ten notatnik to praktyczne wprowadzenie do jednego z pierwszych krokow przetwarzania tekstu przez komputer -- **tokenizacji**. Zanim komputer bedzie mogl zrozumiec tekst (np. przeanalizowac opinie klientow, przetlumaczyc zdanie, odpowiedziec na pytanie), musi ten tekst najpierw podzielic na mniejsze czesci. Wlasnie tym zajmuje sie tokenizacja.\n",
        "\n",
        "Bedziemy korzystac z biblioteki **NLTK** (Natural Language Toolkit) -- jednego z najstarszych i najbardziej sprawdzonych narzedzi do pracy z tekstem w Pythonie. NLTK powstal na uniwersytecie w Pensylwanii i jest powszechnie stosowany w nauczaniu oraz w praktyce.\n",
        "\n",
        "### Plan pracy\n",
        "\n",
        "1. Instalacja i konfiguracja srodowiska\n",
        "2. Czym jest tokenizacja -- definicja i intuicja\n",
        "3. Tokenizacja na wyrazy (word tokenization)\n",
        "4. Tokenizacja na zdania (sentence tokenization)\n",
        "5. Tokenizacja tekstu polskiego\n",
        "6. Rozne tokenizatory w NLTK -- porownanie\n",
        "7. Cwiczenia podsumowujace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK7eilBiLFdb"
      },
      "source": [
        "---\n",
        "## 1. Instalacja i konfiguracja srodowiska\n",
        "\n",
        "Google Colab ma juz zainstalowany pakiet NLTK, ale musimy jeszcze pobrac dodatkowe dane, z ktorych korzystaja tokenizatory (tzw. modele i reguly podzialu tekstu). Ponizszy kod to robi automatycznie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7cN6saHLFdb",
        "outputId": "994ee5fa-f91a-4bf7-d052-16d16caa267f"
      },
      "source": [
        "# Instalacja NLTK (na wypadek gdyby nie byl dostepny)\n",
        "!pip install nltk --quiet\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Pobranie danych potrzebnych do tokenizacji\n",
        "# 'punkt_tab' to zestaw regul, dzieki ktorym NLTK wie,\n",
        "# gdzie konczy sie jedno zdanie i zaczyna kolejne\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "print(f\"Wersja NLTK: {nltk.__version__}\")\n",
        "print(\"Srodowisko gotowe do pracy.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wersja NLTK: 3.9.1\n",
            "Srodowisko gotowe do pracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWfoJLazLFdc"
      },
      "source": [
        "**Co sie wlasnie stalo?**\n",
        "\n",
        "- `pip install nltk` -- instaluje biblioteke (w Colab jest juz obecna, ale komenda nie zaszkodzi).\n",
        "- `nltk.download('punkt_tab')` -- pobiera dane treningowe dla tokenizatora Punkt. To zestaw regul stworzony na podstawie analizy duzych ilosci tekstu. Dzieki niemu NLTK wie na przyklad, ze kropka po \"dr\" nie konczy zdania, ale kropka po \"domu\" -- juz tak."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAVlpjyCLFdc"
      },
      "source": [
        "---\n",
        "## 2. Czym jest tokenizacja?\n",
        "\n",
        "### Definicja\n",
        "\n",
        "**Tokenizacja** (ang. *tokenization*) to proces dzielenia tekstu na mniejsze jednostki zwane **tokenami** (ang. *tokens*). Token to najmniejszy fragment tekstu, ktory ma znaczenie w danym kontekscie -- moze to byc pojedynczy wyraz, znak interpunkcyjny, liczba, a nawet czesc wyrazu.\n",
        "\n",
        "### Dlaczego to jest potrzebne?\n",
        "\n",
        "Komputer nie czyta tekstu tak jak czlowiek. Dla komputera tekst to po prostu ciag znakow (liter, spacji, kropek). Zeby moc cos z tym tekstem zrobic -- policzyc wyrazy, znalezc najczesciej uzywane slowa, przetlumaczyc zdanie -- komputer musi najpierw wiedziec, gdzie zaczyna sie i konczy kazdy wyraz i kazde zdanie.\n",
        "\n",
        "Tokenizacja to pierwszy krok w niemal kazdym procesie przetwarzania jezyka naturalnego (NLP -- Natural Language Processing).\n",
        "\n",
        "### Analogia\n",
        "\n",
        "Wyobraz sobie, ze dostajesz tekst zapisany bez spacji: `Alamakotakotmalape`. Zanim bedziesz mogl zrozumiec to zdanie, musisz je podzielic na wyrazy: `Ala ma kota kot ma lape`. Tokenizacja robi dokladnie to samo, tyle ze w sposob zautomatyzowany i z uwzglednieniem skomplikowanych regul jezykowych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU333SnpLFdc"
      },
      "source": [
        "### Cwiczenie na kartce (offline)\n",
        "\n",
        "Zanim przejdziemy do kodu, sprobuj recznie wykonac tokenizacje. Wez kartke i dlugopis.\n",
        "\n",
        "**Zadanie:** Podziel ponizsze zdanie na tokeny (wyrazy i znaki interpunkcyjne) -- zapisz kazdy token osobno, np. w ramce lub krazku:\n",
        "\n",
        "> Dr. Smith bought 3 books for $12.99 each.\n",
        "\n",
        "Pytania do przemyslenia:\n",
        "- Czy `Dr.` to jeden token, czy dwa (`Dr` + `.`)?\n",
        "- Czy `$12.99` to jeden token, czy kilka?\n",
        "- Ile tokenow w sumie naliczylas/naliczylesz?\n",
        "\n",
        "Zapisz swoje odpowiedzi -- za chwile porownamy je z wynikiem komputera.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3J8gqhtLFdc"
      },
      "source": [
        "## 3. Tokenizacja na wyrazy (Word Tokenization)\n",
        "\n",
        "### Czym jest token wyrazowy?\n",
        "\n",
        "**Token wyrazowy** to najczesciej pojedynczy wyraz lub znak interpunkcyjny wyodrebniony z tekstu. Funkcja `word_tokenize()` w NLTK dzieli tekst na takie jednostki.\n",
        "\n",
        "Wazne: tokenizacja to nie to samo co zwykle dzielenie po spacjach (metoda `split()`). Tokenizator uwzglednia reguly jezykowe -- wie na przyklad, ze `don't` w angielskim to tak naprawde dwa tokeny: `do` + `n't`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po1H39g5LFdd"
      },
      "source": [
        "### 3.1 Pierwszy przyklad -- tekst angielski"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF0-xtpzLFdd",
        "outputId": "d9eed647-e74a-44a4-9c30-5ea817fee291"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Prosty tekst angielski\n",
        "english_text = \"The cat sat on the mat. It didn't move at all.\"\n",
        "\n",
        "# Tokenizacja na wyrazy\n",
        "english_tokens = word_tokenize(english_text)\n",
        "\n",
        "print(\"Tekst zrodlowy:\")\n",
        "print(english_text)\n",
        "print()\n",
        "print(\"Tokeny:\")\n",
        "print(english_tokens)\n",
        "print()\n",
        "print(f\"Liczba tokenow: {len(english_tokens)}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst zrodlowy:\n",
            "The cat sat on the mat. It didn't move at all.\n",
            "\n",
            "Tokeny:\n",
            "['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'It', 'did', \"n't\", 'move', 'at', 'all', '.']\n",
            "\n",
            "Liczba tokenow: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONtOTo5lLFdd"
      },
      "source": [
        "**Analiza wyniku:**\n",
        "\n",
        "Zwroc uwage na kilka rzeczy:\n",
        "- Kropki (`.`) zostaly wyodrebnione jako osobne tokeny.\n",
        "- Slowo `didn't` zostalo podzielone na `did` i `n't` -- to jest poprawne z punktu widzenia gramatyki angielskiej, bo `didn't` = `did not`.\n",
        "- Kazdy wyraz to osobny element listy.\n",
        "\n",
        "### Porownanie z naiwnym podejsciem\n",
        "\n",
        "Zobaczmy, co by sie stalo, gdybysmy uzyliPythonowej metody `split()` zamiast tokenizatora:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXD1Y0WGLFdd",
        "outputId": "586385fa-2972-4640-9aca-bfff70f93b92"
      },
      "source": [
        "# Porownanie: split() vs word_tokenize()\n",
        "english_text = \"The cat sat on the mat. It didn't move at all.\"\n",
        "\n",
        "naive_split = english_text.split()\n",
        "nltk_tokens = word_tokenize(english_text)\n",
        "\n",
        "print(\"Metoda split() -- dzielenie po spacjach:\")\n",
        "print(naive_split)\n",
        "print(f\"Liczba elementow: {len(naive_split)}\")\n",
        "print()\n",
        "print(\"Metoda word_tokenize() -- tokenizacja NLTK:\")\n",
        "print(nltk_tokens)\n",
        "print(f\"Liczba tokenow: {len(nltk_tokens)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metoda split() -- dzielenie po spacjach:\n",
            "['The', 'cat', 'sat', 'on', 'the', 'mat.', 'It', \"didn't\", 'move', 'at', 'all.']\n",
            "Liczba elementow: 11\n",
            "\n",
            "Metoda word_tokenize() -- tokenizacja NLTK:\n",
            "['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'It', 'did', \"n't\", 'move', 'at', 'all', '.']\n",
            "Liczba tokenow: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkZu8SaBLFdd"
      },
      "source": [
        "**Roznice:**\n",
        "\n",
        "- `split()` zostawia `mat.` i `all.` jako jeden element -- kropka jest przyklejona do wyrazu.\n",
        "- `split()` nie rozdziela `didn't` na czesci skladowe.\n",
        "- `word_tokenize()` traktuje znaki interpunkcyjne jako osobne tokeny, co jest istotne przy dalszej analizie tekstu.\n",
        "\n",
        "To jest wlasnie powod, dla ktorego uzywamy wyspecjalizowanych narzedzi zamiast prostego dzielenia po spacjach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNUSLPGKLFdd"
      },
      "source": [
        "### 3.2 Bardziej zlozone przypadki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXywnxkfLFdd",
        "outputId": "9aa4ba08-b3c4-4eec-aefe-0ec74694dada"
      },
      "source": [
        "# Tekst z roznego rodzaju trudnymi przypadkami\n",
        "complex_text = \"Dr. Smith bought 3 books for $12.99 each. He's happy!\"\n",
        "\n",
        "complex_tokens = word_tokenize(complex_text)\n",
        "\n",
        "print(\"Tekst zrodlowy:\")\n",
        "print(complex_text)\n",
        "print()\n",
        "print(\"Tokeny:\")\n",
        "print(complex_tokens)\n",
        "print()\n",
        "\n",
        "# Wyswietlmy tokeny z ich indeksami, zeby lepiej bylo widac\n",
        "print(\"Tokeny z numeracja:\")\n",
        "for index, token in enumerate(complex_tokens):\n",
        "    print(f\"  [{index}] '{token}'\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst zrodlowy:\n",
            "Dr. Smith bought 3 books for $12.99 each. He's happy!\n",
            "\n",
            "Tokeny:\n",
            "['Dr.', 'Smith', 'bought', '3', 'books', 'for', '$', '12.99', 'each', '.', 'He', \"'s\", 'happy', '!']\n",
            "\n",
            "Tokeny z numeracja:\n",
            "  [0] 'Dr.'\n",
            "  [1] 'Smith'\n",
            "  [2] 'bought'\n",
            "  [3] '3'\n",
            "  [4] 'books'\n",
            "  [5] 'for'\n",
            "  [6] '$'\n",
            "  [7] '12.99'\n",
            "  [8] 'each'\n",
            "  [9] '.'\n",
            "  [10] 'He'\n",
            "  [11] ''s'\n",
            "  [12] 'happy'\n",
            "  [13] '!'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4OKBBWiLFdd"
      },
      "source": [
        "**Co warto zauwazyc:**\n",
        "\n",
        "- `Dr.` -- tokenizator wie, ze to skrot i nie traktuje kropki jako konca zdania.\n",
        "- `$` -- symbol waluty jest osobnym tokenem.\n",
        "- `12.99` -- liczba dziesietna pozostaje jako jeden token (kropka nie jest tu koncowym znakiem zdania).\n",
        "- `He's` -- zostaje podzielone na `He` i `'s`, co odzwierciedla gramatyke (`He is` lub `He has`).\n",
        "- `!` -- wykrzyknik to osobny token.\n",
        "\n",
        "Wrocmy teraz do cwiczenia na kartce -- porownaj swoje odpowiedzi z wynikiem powyzej. Ile tokenow sie zgadzalo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my6WhYkELFde"
      },
      "source": [
        "### Cwiczenia do samodzielnego wykonania\n",
        "\n",
        "**Cwiczenie 3.1:** Stokenizuj ponizsze zdanie i policz, ile tokenow uzyskasz. Zanim uruchomisz kod, sprobuj zgadnac wynik.\n",
        "\n",
        "```\n",
        "I've been working at OpenAI since Jan. 2020, and it's been great!\n",
        "```\n",
        "\n",
        "**Cwiczenie 3.2:** Napisz kod, ktory przyjmie dowolny tekst od uzytkownika (mozesz uzyc zmiennej ze stringiem), stokenizuje go, a nastepnie wyswietli tokeny ponumerowane od 1 (nie od 0).\n",
        "\n",
        "**Cwiczenie 3.3:** Porownaj wyniki `split()` i `word_tokenize()` dla zdania: `\"Mrs. O'Brien can't believe it's 3:45 p.m. already!\"`. Zapisz na kartce, ile tokenow przewidujesz w kazdej metodzie, a potem sprawdz kodem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK4iyFiZLFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 3.1\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQz5ESQxLFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 3.2\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMtKqKNLLFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 3.3\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_EhXATsLFde"
      },
      "source": [
        "---\n",
        "## 4. Tokenizacja na zdania (Sentence Tokenization)\n",
        "\n",
        "### Definicja\n",
        "\n",
        "**Tokenizacja na zdania** (ang. *sentence tokenization* lub *sentence segmentation*) to podzial tekstu na poszczegolne zdania. Moze sie to wydawac proste -- wystarczy dzielic po kropkach, prawda? Nie do konca.\n",
        "\n",
        "Kropka pelni w tekscie wiele funkcji:\n",
        "- konczy zdanie (\"Kot spi.\")\n",
        "- jest czescia skrotu (\"dr.\", \"prof.\", \"Jan.\")\n",
        "- jest czescia liczby (\"3.14\")\n",
        "- jest czescia adresu URL (\"www.google.com\")\n",
        "- jest czescia adresu email (\"jan@firma.pl\")\n",
        "\n",
        "Tokenizator `sent_tokenize()` w NLTK radzi sobie z wiekszocia tych przypadkow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pdhpRxPLFde",
        "outputId": "60a12d61-1239-402a-af50-6b118b5e0fce"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tekst angielski z kilkoma zdaniami\n",
        "english_paragraph = (\n",
        "    \"Dr. Smith went to Washington. He arrived on Jan. 5th. \"\n",
        "    \"The meeting was scheduled for 3 p.m. and lasted two hours. \"\n",
        "    \"It was a productive day! Was it worth the trip? Absolutely.\"\n",
        ")\n",
        "\n",
        "english_sentences = sent_tokenize(english_paragraph)\n",
        "\n",
        "print(\"Tekst zrodlowy:\")\n",
        "print(english_paragraph)\n",
        "print()\n",
        "print(f\"Liczba wyodrenionych zdan: {len(english_sentences)}\")\n",
        "print()\n",
        "print(\"Poszczegolne zdania:\")\n",
        "for i, sentence in enumerate(english_sentences, start=1):\n",
        "    print(f\"  Zdanie {i}: {sentence}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst zrodlowy:\n",
            "Dr. Smith went to Washington. He arrived on Jan. 5th. The meeting was scheduled for 3 p.m. and lasted two hours. It was a productive day! Was it worth the trip? Absolutely.\n",
            "\n",
            "Liczba wyodrenionych zdan: 6\n",
            "\n",
            "Poszczegolne zdania:\n",
            "  Zdanie 1: Dr. Smith went to Washington.\n",
            "  Zdanie 2: He arrived on Jan. 5th.\n",
            "  Zdanie 3: The meeting was scheduled for 3 p.m. and lasted two hours.\n",
            "  Zdanie 4: It was a productive day!\n",
            "  Zdanie 5: Was it worth the trip?\n",
            "  Zdanie 6: Absolutely.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UafXOz8PLFde"
      },
      "source": [
        "**Analiza wyniku:**\n",
        "\n",
        "- `Dr.` -- tokenizator nie podzielil zdania po tej kropce, bo rozpoznal skrot.\n",
        "- `Jan.` -- podobnie, skrot miesiaca nie spowodowal podzialu.\n",
        "- `p.m.` -- skrot czasu rowniez jest rozpoznawany.\n",
        "- `!` i `?` -- poprawnie rozpoznane jako koniec zdania.\n",
        "\n",
        "Tokenizator Punkt (uzywany domyslnie przez NLTK) zostal wytrenowany na duzych zbiorach tekstu i zna typowe skroty w jezyku angielskim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IauOe6ZzLFde"
      },
      "source": [
        "### Cwiczenie na kartce (offline)\n",
        "\n",
        "Wez ponizszy tekst i na kartce zaznacz pionowa kreska (`|`) miejsca, w ktorych tekst powinien byc podzielony na zdania:\n",
        "\n",
        "> Prof. Kowalski pracowal w USA. Wrócil w 2019 r. i zaczal wykladac na UW. Czy to prawda? Tak, to potwierdzone.\n",
        "\n",
        "Ile zdan naliczylas/naliczylesz? Ktore kropki sa koncem zdania, a ktore czescia skrotu?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP6FXztGLFde"
      },
      "source": [
        "### Łączenie obu typów tokenizacji\n",
        "\n",
        "W praktyce czesto najpierw dzielimy tekst na zdania, a potem kazde zdanie na wyrazy. To daje nam dwupoziomowa strukture, przydatna w wielu zastosowaniach NLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-obQwwLiLFde",
        "outputId": "b970bdc0-09fe-4a0a-a55b-7fb79fbf574e"
      },
      "source": [
        "# Laczenie tokenizacji na zdania i na wyrazy\n",
        "sample_text = (\n",
        "    \"Natural language processing is a fascinating field. \"\n",
        "    \"It combines linguistics and computer science. \"\n",
        "    \"Many modern applications rely on NLP.\"\n",
        ")\n",
        "\n",
        "sentences = sent_tokenize(sample_text)\n",
        "\n",
        "print(\"Dwupoziomowa tokenizacja:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, sentence in enumerate(sentences, start=1):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(f\"\\nZdanie {i}: {sentence}\")\n",
        "    print(f\"  Tokeny ({len(tokens)}): {tokens}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dwupoziomowa tokenizacja:\n",
            "==================================================\n",
            "\n",
            "Zdanie 1: Natural language processing is a fascinating field.\n",
            "  Tokeny (8): ['Natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', '.']\n",
            "\n",
            "Zdanie 2: It combines linguistics and computer science.\n",
            "  Tokeny (7): ['It', 'combines', 'linguistics', 'and', 'computer', 'science', '.']\n",
            "\n",
            "Zdanie 3: Many modern applications rely on NLP.\n",
            "  Tokeny (7): ['Many', 'modern', 'applications', 'rely', 'on', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBlOKyRKLFde"
      },
      "source": [
        "**Dlaczego to jest przydatne?**\n",
        "\n",
        "Dzieki takiej strukturze mozemy pozniej np. analizowac kazde zdanie z osobna -- policzyc srednia dlugosc zdania, znalezc najdluzsze zdanie w tekscie, albo przeanalizowac, jakie slowa wystepuja najczesciej w pierwszych zdaniach artykulow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6tlDIhLLFde"
      },
      "source": [
        "### Cwiczenia do samodzielnego wykonania\n",
        "\n",
        "**Cwiczenie 4.1:** Uzyj `sent_tokenize()` na ponizszym tekscie i sprawdz, czy tokenizator poprawnie radzi sobie z wielokropkiem i nawiasami:\n",
        "\n",
        "```\n",
        "The experiment failed... Again. The results (see Fig. 2) were inconclusive. We need more data.\n",
        "```\n",
        "\n",
        "**Cwiczenie 4.2:** Napisz funkcje `analyze_text(text)`, ktora przyjmuje tekst i wyswietla: liczbe zdan, liczbe tokenow (wyrazow) oraz srednia liczbe tokenow na zdanie.\n",
        "\n",
        "**Cwiczenie 4.3:** Zastosuj dwupoziomowa tokenizacje do dowolnego akapitu z angielskiej Wikipedii (skopiuj 3-4 zdania). Wyswietl wyniki w czytelnej formie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QltD1Xd4LFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 4.1\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBszzowNLFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 4.2\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmbIoB5KLFde"
      },
      "source": [
        "# Miejsce na Cwiczenie 4.3\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM4vR9rZLFdf"
      },
      "source": [
        "---\n",
        "## 5. Tokenizacja tekstu polskiego\n",
        "\n",
        "### Specyfika jezyka polskiego\n",
        "\n",
        "Jezyk polski ma kilka cech, ktore odrozniaja go od angielskiego w kontekscie tokenizacji:\n",
        "\n",
        "- **Odmiana wyrazow** -- wyraz \"dom\" moze wystapic jako \"domu\", \"domowi\", \"domem\", \"domy\" itd. Kazda forma to osobny token, mimo ze chodzi o to samo slowo.\n",
        "- **Znaki diakrytyczne** -- litery ą, ć, ę, ł, ń, ó, ś, ż, ż. Tokenizator musi je poprawnie obslugiwac.\n",
        "- **Skroty** -- polskie skroty (\"prof.\", \"dr hab.\", \"ul.\", \"r.\") roznia sie od angielskich.\n",
        "- **Brak form sciagnionych** -- w polskim nie mamy odpowiednikow `don't` czy `it's`, wiec ten problem nas nie dotyczy.\n",
        "\n",
        "NLTK obsluguje jezyk polski -- przy wywolaniu tokenizatora mozemy wskazac jezyk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqXE03kjLFdf",
        "outputId": "667e4b2c-4a1f-4f00-e2ee-a72a95401abd"
      },
      "source": [
        "# Tokenizacja polskiego tekstu -- wyrazy\n",
        "polish_text = (\n",
        "    \"Prof. Nowak przyjechała do Wrocławia. \"\n",
        "    \"Spotkanie odbyło sie o godz. 14:30 w sali nr 205. \"\n",
        "    \"Czy wykład sie odbędzie? Oczywiście!\"\n",
        ")\n",
        "\n",
        "# Tokenizacja na wyrazy -- wskazujemy jezyk polski\n",
        "polish_tokens = word_tokenize(polish_text, language='polish')\n",
        "\n",
        "print(\"Tekst zrodlowy:\")\n",
        "print(polish_text)\n",
        "print()\n",
        "print(\"Tokeny:\")\n",
        "print(polish_tokens)\n",
        "print(f\"\\nLiczba tokenow: {len(polish_tokens)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst zrodlowy:\n",
            "Prof. Nowak przyjechała do Wrocławia. Spotkanie odbyło sie o godz. 14:30 w sali nr 205. Czy wykład sie odbędzie? Oczywiście!\n",
            "\n",
            "Tokeny:\n",
            "['Prof.', 'Nowak', 'przyjechała', 'do', 'Wrocławia', '.', 'Spotkanie', 'odbyło', 'sie', 'o', 'godz.', '14:30', 'w', 'sali', 'nr', '205', '.', 'Czy', 'wykład', 'sie', 'odbędzie', '?', 'Oczywiście', '!']\n",
            "\n",
            "Liczba tokenow: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA-IhEVLLFdf",
        "outputId": "6c9eef28-4da2-47f1-9973-5f4fb0f974fb"
      },
      "source": [
        "# Tokenizacja polskiego tekstu -- zdania\n",
        "polish_paragraph = (\n",
        "    \"Dr hab. Jan Kowalski pracował na Politechnice Warszawskiej. \"\n",
        "    \"W 2018 r. opublikował 5 artykułów naukowych. \"\n",
        "    \"Jeden z nich dotyczy przetwarzania jezyka naturalnego. \"\n",
        "    \"Czy to ważny temat? Bez wątpienia.\"\n",
        ")\n",
        "\n",
        "polish_sentences = sent_tokenize(polish_paragraph, language='polish')\n",
        "\n",
        "print(\"Tekst zrodlowy:\")\n",
        "print(polish_paragraph)\n",
        "print()\n",
        "print(f\"Wykryte zdania ({len(polish_sentences)}):\")\n",
        "for i, sent in enumerate(polish_sentences, start=1):\n",
        "    print(f\"  {i}. {sent}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tekst zrodlowy:\n",
            "Dr hab. Jan Kowalski pracował na Politechnice Warszawskiej. W 2018 r. opublikował 5 artykułów naukowych. Jeden z nich dotyczy przetwarzania jezyka naturalnego. Czy to ważny temat? Bez wątpienia.\n",
            "\n",
            "Wykryte zdania (5):\n",
            "  1. Dr hab. Jan Kowalski pracował na Politechnice Warszawskiej.\n",
            "  2. W 2018 r. opublikował 5 artykułów naukowych.\n",
            "  3. Jeden z nich dotyczy przetwarzania jezyka naturalnego.\n",
            "  4. Czy to ważny temat?\n",
            "  5. Bez wątpienia.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbJlc6n8LFdf"
      },
      "source": [
        "**Uwagi:**\n",
        "\n",
        "- Parametr `language='polish'` informuje tokenizator, ze pracujemy z tekstem polskim. Dzieki temu korzysta z regul wlasciwych dla polszczyzny.\n",
        "- Tokenizator moze miec problemy z niektorymi polskimi skrotami (np. \"dr hab.\", \"r.\"). To jest ograniczenie NLTK -- w praktyce warto sprawdzac wyniki i byc swiadomym, ze zadne narzedzie nie jest doskonale.\n",
        "- Dla zaawansowanego przetwarzania polskiego tekstu istnieja dedykowane narzedzia, np. Stanza lub spaCy z modelem polskim. NLTK to jednak dobry punkt wyjscia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYIIXfcrLFdf"
      },
      "source": [
        "### Cwiczenie na kartce (offline)\n",
        "\n",
        "Wez ponizsze zdanie i recznie podziel je na tokeny. Zapisz kazdy token w osobnej ramce:\n",
        "\n",
        "> Inz. Wisniewska mieszka przy ul. Dlugiej 12/4 w Krakowie.\n",
        "\n",
        "Pytania:\n",
        "- Jak potraktowales/potraktowalas \"ul.\"? Czy to jeden token, czy dwa?\n",
        "- A \"12/4\"? Jeden token, czy trzy (\"12\", \"/\", \"4\")?\n",
        "- Jak myslisz, co zrobi z tym NLTK? Sprawdz kodem ponizej."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbgbn99MLFdf",
        "outputId": "501b4a9a-4c48-498e-9aaa-5e51f550ab82"
      },
      "source": [
        "# Sprawdzenie cwiczenia z kartki\n",
        "address_text = \"Inz. Wisniewska mieszka przy ul. Dlugiej 12/4 w Krakowie.\"\n",
        "\n",
        "address_tokens = word_tokenize(address_text, language='polish')\n",
        "\n",
        "print(\"Tokeny:\")\n",
        "for i, token in enumerate(address_tokens, start=1):\n",
        "    print(f\"  [{i:2d}] '{token}'\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokeny:\n",
            "  [ 1] 'Inz'\n",
            "  [ 2] '.'\n",
            "  [ 3] 'Wisniewska'\n",
            "  [ 4] 'mieszka'\n",
            "  [ 5] 'przy'\n",
            "  [ 6] 'ul.'\n",
            "  [ 7] 'Dlugiej'\n",
            "  [ 8] '12/4'\n",
            "  [ 9] 'w'\n",
            "  [10] 'Krakowie'\n",
            "  [11] '.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOH3UvKLFdf"
      },
      "source": [
        "### Cwiczenia do samodzielnego wykonania\n",
        "\n",
        "**Cwiczenie 5.1:** Stokenizuj ponizszy polski tekst na zdania i wyrazy. Wyswietl wyniki w formie dwupoziomowej (jak w rozdziale 4):\n",
        "\n",
        "```\n",
        "Warszawa jest stolica Polski. Liczy ok. 1,8 mln mieszkancow. W miescie znajduje sie wiele zabytkow, m.in. Zamek Krolewski i Lazienki.\n",
        "```\n",
        "\n",
        "**Cwiczenie 5.2:** Porownaj wyniki tokenizacji tego samego tekstu z parametrem `language='polish'` i bez niego (domyslnie uzyty zostanie angielski). Czy widzisz roznice?\n",
        "\n",
        "**Cwiczenie 5.3:** Napisz funkcje `count_unique_tokens(text, language)`, ktora zwraca liczbe unikalnych tokenow w tekscie (wskazowka: uzyj `set()`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMU9lJl8LFdf"
      },
      "source": [
        "# Miejsce na Cwiczenie 5.1\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s3WbfUPLFdf"
      },
      "source": [
        "# Miejsce na Cwiczenie 5.2\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbDoLFBfLFdi"
      },
      "source": [
        "# Miejsce na Cwiczenie 5.3\n",
        "# Wpisz swoj kod ponizej:\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}